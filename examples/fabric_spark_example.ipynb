{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a69d2f",
   "metadata": {},
   "source": [
    "# Fabric Spark Job & Notebook with LakeTrace Logging\n",
    "\n",
    "This notebook demonstrates how to use **LakeTrace** logging in Microsoft Fabric:\n",
    "- Structured JSON logging with automatic rotation\n",
    "- Safe for both Notebooks and Spark Job Definitions\n",
    "- Automatic runtime detection (Fabric vs local)\n",
    "- End-of-run lakehouse upload capability\n",
    "- Thread-safe, Spark-driver-safe logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50917033",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Import PySpark\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, concat_ws, lit, current_timestamp\n",
    "\n",
    "# Import LakeTrace\n",
    "try:\n",
    "    from laketrace import get_logger\n",
    "    from laketrace.runtime import detect_runtime\n",
    "except ImportError:\n",
    "    # Fallback for local testing\n",
    "    print(\"Warning: laketrace not installed. Install with: pip install laketrace-logger\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875cd2b1",
   "metadata": {},
   "source": [
    "## Section 2: Initialize LakeTrace Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique run ID for this execution\n",
    "run_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "# Detect runtime environment (Fabric, Databricks, or local)\n",
    "runtime_info = detect_runtime()\n",
    "\n",
    "# Configure logger for Fabric/Spark environment\n",
    "logger_config = {\n",
    "    \"log_dir\": \"/tmp/laketrace_logs\",  # Local temp dir (safe for Spark)\n",
    "    \"rotation_mb\": 10,                  # Rotate every 10 MB\n",
    "    \"retention_files\": 5,               # Keep 5 rotated files\n",
    "    \"level\": \"INFO\",\n",
    "    \"json\": True,                       # Structured JSON output\n",
    "    \"stdout\": True,                     # Also emit to stdout\n",
    "    \"compression\": \"gz\",                # Compress rotated logs\n",
    "    \"add_runtime_context\": True,        # Include runtime metadata\n",
    "}\n",
    "\n",
    "# Initialize logger with config\n",
    "logger = get_logger(\n",
    "    name=\"fabric_pipeline\",\n",
    "    config=logger_config\n",
    ")\n",
    "\n",
    "# Bind context fields that will be included in all logs\n",
    "logger = logger.bind(\n",
    "    run_id=run_id,\n",
    "    stage=\"initialization\",\n",
    "    notebook=\"fabric_spark_example\",\n",
    "    runtime=runtime_info.get(\"type\", \"unknown\")\n",
    ")\n",
    "\n",
    "logger.info(\"Logger initialized\", \n",
    "    runtime_type=runtime_info.get(\"type\"),\n",
    "    platform=runtime_info.get(\"platform\")\n",
    ")\n",
    "print(f\"✓ LakeTrace logger ready (run_id={run_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db830e71",
   "metadata": {},
   "source": [
    "## Section 3: Create Spark Session for Fabric\n",
    "\n",
    "In Fabric notebooks, SparkSession is pre-initialized. For Spark Job definitions, we get the active session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get or create Spark session\n",
    "try:\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is None:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"fabric-pipeline\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.parquet.columnarReadEnabled\", \"true\") \\\n",
    "            .getOrCreate()\n",
    "        logger.info(\"Created new Spark session\")\n",
    "    else:\n",
    "        logger.info(\"Using active Spark session\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize Spark: {str(e)}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# Log Spark configuration\n",
    "logger.info(\"Spark session ready\",\n",
    "    app_name=spark.sparkContext.appName,\n",
    "    master=spark.sparkContext.master,\n",
    "    version=spark.__version__\n",
    ")\n",
    "print(f\"✓ Spark session initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2d6fa",
   "metadata": {},
   "source": [
    "## Section 4: Load Sample Data into Spark DataFrame\n",
    "\n",
    "Create sample data to simulate a typical data pipeline scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41750d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update logger stage\n",
    "logger = logger.bind(stage=\"data_loading\")\n",
    "\n",
    "try:\n",
    "    # Create sample data\n",
    "    data = [\n",
    "        {\"id\": 1, \"name\": \"Alice\", \"department\": \"Sales\", \"salary\": 50000},\n",
    "        {\"id\": 2, \"name\": \"Bob\", \"department\": \"Engineering\", \"salary\": 75000},\n",
    "        {\"id\": 3, \"name\": \"Charlie\", \"department\": \"Sales\", \"salary\": 55000},\n",
    "        {\"id\": 4, \"name\": \"Diana\", \"department\": \"Engineering\", \"salary\": 80000},\n",
    "        {\"id\": 5, \"name\": \"Eve\", \"department\": \"HR\", \"salary\": 60000},\n",
    "    ]\n",
    "    \n",
    "    df = spark.createDataFrame(data)\n",
    "    \n",
    "    logger.info(\"Sample data loaded\",\n",
    "        record_count=len(data),\n",
    "        columns=df.columns,\n",
    "        schema=str(df.schema)\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Loaded {len(data)} records\")\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load data: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb000d5",
   "metadata": {},
   "source": [
    "## Section 5: Perform Spark Transformations with Logging\n",
    "\n",
    "Apply transformations and log key metrics at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update logger stage\n",
    "logger = logger.bind(stage=\"transformation\")\n",
    "\n",
    "try:\n",
    "    # Transformation 1: Filter high earners\n",
    "    logger.debug(\"Starting transformation: filter salary > 60000\")\n",
    "    high_earners = df.filter(col(\"salary\") > 60000)\n",
    "    high_earners_count = high_earners.count()\n",
    "    \n",
    "    logger.info(\"Filtered high earners\",\n",
    "        filter_condition=\"salary > 60000\",\n",
    "        result_count=high_earners_count\n",
    "    )\n",
    "    \n",
    "    # Transformation 2: Add salary tier column\n",
    "    logger.debug(\"Starting transformation: add salary tier\")\n",
    "    df_with_tier = high_earners.withColumn(\n",
    "        \"salary_tier\",\n",
    "        col(\"salary\").cast(\"int\")\n",
    "    ).withColumn(\n",
    "        \"processed_at\",\n",
    "        current_timestamp()\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Added computed columns\",\n",
    "        columns_added=[\"salary_tier\", \"processed_at\"]\n",
    "    )\n",
    "    \n",
    "    # Transformation 3: Group and aggregate\n",
    "    logger.debug(\"Starting transformation: department aggregation\")\n",
    "    dept_stats = df_with_tier.groupBy(\"department\").agg({\n",
    "        \"salary\": \"avg\",\n",
    "        \"id\": \"count\"\n",
    "    }).withColumnRenamed(\"avg(salary)\", \"avg_salary\") \\\n",
    "     .withColumnRenamed(\"count(id)\", \"emp_count\")\n",
    "    \n",
    "    dept_count = dept_stats.count()\n",
    "    logger.info(\"Department aggregation complete\",\n",
    "        departments=dept_count,\n",
    "        agg_type=\"avg_salary,emp_count\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Transformations complete\")\n",
    "    dept_stats.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Transformation failed: {str(e)}\", \n",
    "        stage=\"transformation\",\n",
    "        exc_info=True\n",
    "    )\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67740279",
   "metadata": {},
   "source": [
    "## Section 6: Write Results to Fabric Lakehouse\n",
    "\n",
    "Save transformed data in Delta format (native to Fabric Lakehouse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b226fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update logger stage\n",
    "logger = logger.bind(stage=\"data_write\")\n",
    "\n",
    "try:\n",
    "    # Define output path (for demonstration, using local path)\n",
    "    # In real Fabric scenario, use: \"abfss://workspace@fabric.dfs.core.windows.net/...\"\n",
    "    output_path = \"/tmp/laketrace_output/dept_stats\"\n",
    "    \n",
    "    logger.info(\"Starting data write\",\n",
    "        output_path=output_path,\n",
    "        format=\"delta\",\n",
    "        mode=\"overwrite\"\n",
    "    )\n",
    "    \n",
    "    # Write with OVERWRITE mode (safe for Spark)\n",
    "    dept_stats.coalesce(1).write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(output_path)\n",
    "    \n",
    "    logger.info(\"Data write complete\",\n",
    "        path=output_path,\n",
    "        rows=dept_stats.count(),\n",
    "        format=\"delta\",\n",
    "        duration_note=\"check logs for actual duration\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Results written to {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Write failed: {str(e)}\",\n",
    "        stage=\"data_write\",\n",
    "        output_path=output_path,\n",
    "        exc_info=True\n",
    "    )\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f1ea3",
   "metadata": {},
   "source": [
    "## Section 7: Submit as Spark Job (Job Definition Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1084421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Spark Job Definition in Fabric:\n",
    "# 1. Copy the code above into a .py file\n",
    "# 2. Configure job parameters via Fabric UI or API:\n",
    "\n",
    "job_config = {\n",
    "    \"name\": \"fabric-data-pipeline\",\n",
    "    \"description\": \"Data pipeline with LakeTrace logging\",\n",
    "    \"compute\": \"fabric_spark_compute\",  # Your compute cluster\n",
    "    \"entry_point\": \"pipeline.py\",\n",
    "    \"parameters\": {\n",
    "        \"log_level\": \"INFO\",\n",
    "        \"environment\": \"production\",\n",
    "        \"run_id\": run_id\n",
    "    },\n",
    "    \"timeout_minutes\": 60,\n",
    "    \"retries\": 1  # Limit retries for stability\n",
    "}\n",
    "\n",
    "logger.info(\"Job configuration for Spark Job Definition\",\n",
    "    job_name=job_config[\"name\"],\n",
    "    entry_point=job_config[\"entry_point\"],\n",
    "    parameters=job_config[\"parameters\"]\n",
    ")\n",
    "\n",
    "print(\"✓ Job can be submitted via Fabric UI or SDK\")\n",
    "print(f\"  Reference run_id: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18344441",
   "metadata": {},
   "source": [
    "## Section 8: Monitor Job Execution & Upload Logs\n",
    "\n",
    "Final step: capture logs and optionally upload to Fabric Lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6362f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update logger stage\n",
    "logger = logger.bind(stage=\"completion\")\n",
    "\n",
    "try:\n",
    "    # Display final log summary\n",
    "    logger.info(\"Pipeline execution complete\",\n",
    "        run_id=run_id,\n",
    "        total_stages=5,\n",
    "        status=\"success\"\n",
    "    )\n",
    "    \n",
    "    # View recent logs\n",
    "    print(\"\\n=== Recent Log Entries ===\")\n",
    "    logger.tail(n=20)  # Show last 20 lines of log file\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Completion step failed: {str(e)}\", exc_info=True)\n",
    "\n",
    "# Optional: Upload logs to Fabric Lakehouse (end-of-run only)\n",
    "try:\n",
    "    # This method only works in Fabric/Databricks with proper auth\n",
    "    # Specify abfss path: \"abfss://workspace@fabric.dfs.core.windows.net/logs/\"\n",
    "    # For local testing, skip or use local path\n",
    "    \n",
    "    lakehouse_path = None  # Set to real path if needed\n",
    "    if lakehouse_path:\n",
    "        logger.info(\"Uploading logs to Fabric Lakehouse\",\n",
    "            target_path=lakehouse_path\n",
    "        )\n",
    "        # logger.upload_log_to_lakehouse(lakehouse_path)\n",
    "        logger.info(\"Log upload complete\")\n",
    "    else:\n",
    "        logger.debug(\"Skipping log upload (no lakehouse path configured)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not upload logs: {str(e)}\")\n",
    "    print(\"Note: Log upload requires Fabric/Databricks environment with auth\")\n",
    "\n",
    "print(\"\\n✓ Pipeline execution finished\")\n",
    "print(f\"  Run ID: {run_id}\")\n",
    "print(f\"  Check /tmp/laketrace_logs/ for detailed logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529a79f",
   "metadata": {},
   "source": [
    "## Key Features Demonstrated\n",
    "\n",
    "### 1. **Fabric-Safe Logging**\n",
    "- No `notebookutils.fs.append` (per-line writes)\n",
    "- No `dbutils.fs.put` in log write path\n",
    "- Local rotation handles everything safely\n",
    "\n",
    "### 2. **Structured Output**\n",
    "- JSON format with timestamp, level, logger name, hostname, PID\n",
    "- Runtime context (Fabric vs Databricks detection)\n",
    "- Bound fields (run_id, stage, notebook name)\n",
    "\n",
    "### 3. **Multiple Sinks**\n",
    "- Local rotating file (auto-compresses on rotation)\n",
    "- Stdout (visible in Fabric job output)\n",
    "- Optional lakehouse upload at end of run\n",
    "\n",
    "### 4. **Spark-Safe Design**\n",
    "- Driver-only logging (executors use print)\n",
    "- No distributed I/O\n",
    "- Thread-safe with Loguru engine\n",
    "\n",
    "### 5. **Production Ready**\n",
    "- Configurable rotation size & retention\n",
    "- Bounded retries (max 2 for upload)\n",
    "- Graceful error handling (non-fatal failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfefd54",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### For Fabric Notebooks:\n",
    "1. Save this notebook to Fabric workspace\n",
    "2. Attach to a Spark cluster\n",
    "3. Run cells sequentially\n",
    "4. Monitor logs in stdout + local file\n",
    "\n",
    "### For Fabric Spark Job Definitions:\n",
    "1. Extract the code logic into `pipeline.py`\n",
    "2. Create a Spark Job Definition in Fabric UI\n",
    "3. Configure job parameters (compute, timeout)\n",
    "4. Schedule or submit manually\n",
    "5. Logs appear in job execution logs + local dir\n",
    "\n",
    "### For Databricks:\n",
    "Same code works identically due to runtime detection. Logs go to driver node `/tmp/laketrace_logs/`.\n",
    "\n",
    "### Configuration Options:\n",
    "- Adjust `rotation_mb`, `retention_files`, `level` in `logger_config`\n",
    "- Set `compression` to \"gz\", \"zip\", or \"none\"\n",
    "- Bind additional context fields with `logger.bind(**fields)`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
